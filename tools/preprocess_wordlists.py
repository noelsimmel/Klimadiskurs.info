# This script preprocesses a list of (potential) glossary terms to conform with glossary rules
# Input/output: a plain text list of terms with one term per line
# The following preprocessing steps are applied:
# 1. Each term is searched in the CCG text corpus (generated by create_master_text_file.py)
#    Only terms that appear at least 2x are kept
# 2. Terms that appear less than 2x are searched on Twitter
#    If they have been tweeted at least 2x, they are kept
# 3. Terms are searched in the Duden online dictionary
#    3.1 Terms that appear in the Duden are discarded
#    3.2 Compounds whose second part is NOT in the duden are discarded
# Each step creates a .txt for its result

from dotenv import load_dotenv
from os import environ
from time import sleep
import re
import requests
import string
import tweepy

def read_file(path):
    """Reads a list from a plain text file into a Python list

    Args:
        path (str): Path to the text file.

    Returns:
        list(str): List of terms.
    """

    wordlist = []
    with open(path, mode="r", encoding="utf-8", errors="replace") as f:
        for line in f:
            wordlist.append(line.strip())
    return wordlist

def write_file(path, wordlist):
    """Writes Python list contents to a plain text file, one item per line.

    Args:
        path (str): Path to the text file (will be created or overwritten.)
        wordlist (list(str)): List of terms to write to file.
    """

    wordlist = sorted(list(wordlist))
    with open(path, mode="w+", encoding="utf-8", errors="replace") as f:
        for word in wordlist:
            f.write(word + "\n")

def search_corpus():
    """Step 1. Searches each term in the corpus (texts_pro.txt and texts_contra.txt). 
    Also does some light preprocessing like removing - and * from terms and deduplicating.
    Follows the same heuristic as __get_examples() in create_json.py.
    Terms that do not appear at least 2x in the corpus are discarded.
    Writes resulting list to raw_data/wordlist_1corpus.txt
    """

    wordlist = read_file(original_list)
    print(f"Found {len(wordlist)} words in original list")

    # delete all hyphens since we only want non-hyphenated words in the final list
    # then remove all duplicates by converting the list to a set
    wordlist = [w.replace("-", "") for w in wordlist]
    # * is a special character in Twitter API so must be removed
    wordlist = [w.replace("*", "") for w in wordlist]
    wordlist = set(wordlist)
    print(f"Removed duplicates, {len(wordlist)} words left")

    # read the full texts and tokenize
    # same code as in create_json.py so example sentences can actually be found later
    full_text = ""
    for path in ("raw_data/texts_pro.txt", "raw_data/texts_contra.txt"):
        with open(path, encoding="utf-8", errors="replace") as f:
            full_text += f.read()
    full_text = full_text.translate(str.maketrans("", "", string.punctuation+"‚„“”‛’‘"))
    sentences = re.split(r"[\.|!|?|…] |…\.|\n", full_text)
    sentences = [s for s in sentences if not s.startswith("*** FILE")
                 and len(s.split()) >= 4
                 and "klima" in s.lower()] 

    print("Counting word frequencies in the corpus...")
    words_in_text = []
    for idx, word in enumerate(wordlist):
        if idx in range(0, len(wordlist), 100):
            print(f"\tSearching word #{idx}")
        matches = set()     # set to avoid duplicate sentences
        # tokenizing + iterating over sentences is faster than re.findall() on the whole text
        for sent in sentences:
            for w in (word, "klima-" + word[5:]):
                if re.search(r"\b"+re.escape(w)+r"\b(?!-)", sent, flags=re.I):
                    matches.add(sent)
            if len(matches) > 1: 
                words_in_text.append(word)
                break
    print(f"Found {len(words_in_text)} words with >1 occurences in the corpus")

    # backup
    write_file("raw_data/wordlist_1corpus.txt", words_in_text)

def search_twitter():
    """Step 2. Searches Twitter for the terms that were NOT found in the corpus.
    Terms that were tweeted at least 2x are added to the list.
    """

    words_in_text = read_file("raw_data/wordlist_1corpus.txt")
    wordlist = read_file(original_list)
    wordlist = [w.replace("-", "") for w in wordlist]
    wordlist = [w.replace("*", "") for w in wordlist]   # * breaks Twitter API calls

    # 2. query twitter for the words that haven't been found more than once
    words_not_in_text = set(wordlist) - set(words_in_text)
    words_on_twitter = []

    # connect to twitter API
    load_dotenv("../klimadiskurs/.env")
    api = tweepy.Client(bearer_token=environ.get("TW_BEARER_TOKEN"), wait_on_rate_limit=True)

    print(f"Searching Twitter for the remaining {len(words_not_in_text)} words...")
    for idx, word in enumerate(words_not_in_text):
        if idx in range(0, len(words_not_in_text), 100):
            print(f"\tQuerying word #{idx}")
        # search whole Twitter archive
        # note that exact phrase matching is too liberal in the Twitter API
        # e.g. "klima-kanzler" also matches "Gespräch über Klima - Kanzler Scholz im Interview"
        # if you want to be 100%, manually check the terms that are not linked in the app 
        tweets = api.search_all_tweets(word+" OR "+"klima-"+word[5:]+" lang:de -is:retweet", 
                                       since_id=20)
        sleep(1)    # avoids Twitter rate limiting
        if tweets and len(tweets) > 1:
            words_on_twitter.append(word)

    write_file("raw_data/wordlist_2twitter.txt", words_on_twitter)
    print(f"Found {len(words_on_twitter)} words with >1 tweets")
    print(f"Total list: {len(words_in_text) + len(words_on_twitter)} words")

def search_duden():
    """Step 3. Search the terms in the Duden dictionary.
    Terms that appear in the Duden are discarded (no need to put them in a glossary).
    Compounds whose second parts are NOT in the Duden are discarded (likely typos or too obscure).
    """
    
    wordlist = read_file("raw_data/wordlist_1corpus.txt")
    wordlist += read_file("raw_data/wordlist_2twitter.txt")

    words_in_duden = []

    print("Checking if compound parts are in the Duden dictionary")
    for idx, word in enumerate(wordlist):
        if idx in range(0, len(wordlist), 100):
            print(f"\tQuerying word #{idx}")

        # replace umlauts like in the Duden URLs
        query = word.replace("ä", "ae").replace("ö", "oe").replace("ü", "ue")
        query = query.replace("ß", "sz")

        # don't include a compound if it itself is in the lexicon
        r = requests.head("https://www.duden.de/rechtschreibung/" + query.capitalize())
        if r.status_code == 200:
            continue
            
        # check only second part of the compound (cut off klima-)
        # capitalize because the second part must also be a noun
        r = requests.head("https://www.duden.de/rechtschreibung/" + query[5:].capitalize())
        if r.status_code == 200:
            words_in_duden.append(word)

    words_in_duden = sorted(words_in_duden)
    write_file("raw_data/wordlist_3duden.txt", words_in_duden)
    print(f"Final result: {len(words_in_duden)} words, list is in /raw_data")


original_list = "raw_data/wordlist.txt"

search_corpus()
search_twitter()
search_duden()
